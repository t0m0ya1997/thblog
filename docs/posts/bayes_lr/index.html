<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>ベイズ線形回帰 | TH's Blog</title>
<meta name=keywords content="ベイズ推論,機械学習,線形回帰,C++,Python">
<meta name=description content="この記事では，ベイズ線形回帰を紹介します．">
<meta name=author content="TH">
<link rel=canonical href=https://t0m0ya1997.github.io/thblog/posts/bayes_lr/>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XG2CN43ZPM"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-XG2CN43ZPM')</script>
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/thblog/assets/css/stylesheet.min.0696e4ce76db54069449988b94e1c3c093e7097a1a5fd76436e2f298d28c0d42.css integrity="sha256-BpbkznbbVAaUSZiLlOHDwJPnCXoaX9dkNuLymNKMDUI=" rel="preload stylesheet" as=style>
<link rel=preload href=images/home.svg as=image>
<script defer crossorigin=anonymous src=/thblog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XG2CN43ZPM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-XG2CN43ZPM',{anonymize_ip:!1})}</script>
<meta property="og:title" content="ベイズ線形回帰">
<meta property="og:description" content="この記事では，ベイズ線形回帰を紹介します．">
<meta property="og:type" content="article">
<meta property="og:url" content="https://t0m0ya1997.github.io/thblog/posts/bayes_lr/"><meta property="og:image" content="https://t0m0ya1997.github.io/thblog/images/profile.webp"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-01-30T00:00:00+00:00">
<meta property="article:modified_time" content="2022-01-30T00:00:00+00:00"><meta property="og:site_name" content="TH's Blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://t0m0ya1997.github.io/thblog/images/profile.webp">
<meta name=twitter:title content="ベイズ線形回帰">
<meta name=twitter:description content="この記事では，ベイズ線形回帰を紹介します．">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://t0m0ya1997.github.io/thblog/posts/"},{"@type":"ListItem","position":2,"name":"ベイズ線形回帰","item":"https://t0m0ya1997.github.io/thblog/posts/bayes_lr/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ベイズ線形回帰","name":"ベイズ線形回帰","description":"この記事では，ベイズ線形回帰を紹介します．","keywords":["ベイズ推論","機械学習","線形回帰","C++","Python"],"articleBody":"修士論文審査の前日にして，特に発表には関係のない記事を書きました． 僕には，試験前，締め切り前，など切羽詰まった課題の前に立たされると，その課題以外の課題に猛烈に興味を持ったり，普段は開かない，随分前に読んだ本を手に取り，ついつい読んでしまうことが多々あります． 今日もそんな感じです．\nこの記事では，ベイズ線形回帰を紹介します． 線形回帰モデルとは，$d\\in\\mathbb N$ 次元入力 $\\bm x\\in\\mathbb R^d$ と回帰係数 $\\bm w^d$ の線形結合で実数出力 $y\\in\\mathbb R$ を予測するモデルで，ノイズを $\\epsilon\\in\\mathbb R$ として，以下の関係を仮定します．\n$$ y = \\bm w^T\\bm x + \\epsilon \\tag{1} $$\nこれは，$d=1$ のとき「単回帰」，$d=2$以上のとき「重回帰」と呼ばれるモデルです． また，入力変数 $\\bm x$ の $m\\in\\mathbb N$ 個の関数値による特徴ベクトル $\\bm \\phi(\\bm x) = (\\phi_1(\\bm x),\\phi_2(\\bm x),\\cdots,\\phi_m(\\bm x))^T$ に対して回帰係数 $\\bm w\\in\\mathbb R^m$ を仮定し，\n$$ y = \\bm w^T\\bm \\phi(\\bm x) + \\epsilon \\tag{2} $$\nとして $\\bm w$ と $\\bm\\phi(\\bm x)$ の線型結合で出力変数を予測することもあります． これもまた，線形回帰と呼ばれます． 上述した単回帰および重回帰は，$\\bm\\phi(\\bm x) = \\bm x$ とした場合であることがわかります．\n前提として，線形回帰モデルでは，「入力と出力の関係を表す回帰係数 $\\bm w$ を良い感じに定める」ことが目標となります．\n 線形回帰モデルの学習 $N\\in\\mathbb R$ 個の入力・出力の組 $\\mathcal D = (\\bm x_n,y_n)_{n=1}^N$ が与えられたとします． このとき，以下の二乗誤差 $E(\\bm w)$ を最小化するように $\\bm w$ を決定することがあります．\n$$ E(\\bm w) = \\frac{1}{2}\\sum_{n=1}^N\\left\\{y_n - \\bm w^T\\bm \\phi(\\bm x_n)\\right\\}^2 \\tag{3} $$\nこれは，最小二乗法と呼ばれ，一度は耳にしたことがあるかもしれません． この最小二乗法で得られる $\\bm w$ は，ノイズがガウス分布に従うと仮定したもとでの最尤推定量 $\\bm w_{ML}$ と一致することが次から分ります．\nまず，便宜的に 入力・出力のデータ $\\mathcal D$ が与えられたもとでの，$\\bm w$ の確率分布 $p(\\bm w|\\mathcal D)$ を仮定します． これを，$\\mathcal D$ を観測したもとでの $\\bm w$ の事後確率/事後分布と呼びます． ベイズの定理から\n$$ p(\\bm w|\\mathcal D) = \\frac{p(\\mathcal D|\\bm w)p(\\bm w)}{p(\\mathcal D)} \\propto p(\\mathcal D|\\bm w)p(\\bm w) \\tag{4} $$\nが得られます． ここで，$p(\\mathcal D|\\bm w)$ は尤度関数，$p(\\bm w)$ は事前確率/事前分布と呼ばれます． 最尤推定とは，尤度関数を最大化させる $\\bm w$ を求めることを指します． では，ノイズが平均 $0$, 分散 $\\sigma_{\\epsilon}^2$ のガウス分布に独立に従うと仮定すると，尤度関数は\n$$ \\begin{aligned} p(\\mathcal D|\\bm w) \u0026 = \\prod_{n=1}^N\\mathcal N(\\epsilon_n|0,\\sigma_{\\epsilon}^2) = \\prod_{n=1}^N\\mathcal N(y_n-\\bm w^T\\bm\\phi(\\bm x_n)|0,\\sigma_{\\epsilon}^2) \\cr \u0026 = (2\\pi\\sigma_{\\epsilon}^2)^{-N/2}\\prod_{n=1}^N\\exp\\left(\\frac{(y_n-\\bm w^T\\bm\\phi(\\bm x_n))^2}{2\\sigma_{\\epsilon}^2}\\right) \\end{aligned} \\tag{5} $$\nとなります． この尤度関数を最大化することは，対数をとり符号反転をした $-\\log p(\\mathcal D|\\bm w)$ を $\\bm w$ について最小化することと等価です． つまり，\n$$ -\\log p(\\mathcal D|\\bm w) = \\frac{1}{2\\sigma^2}\\sum_{n=1}^N\\left\\{y_n-\\bm w^T\\bm\\phi(\\bm x_n)\\right\\}^2 + \\frac{N}{2}\\log(2\\pi\\sigma^2) \\tag{6} $$\nを $\\bm w$ について最小化することと等価です． この式の $\\bm w$ に非依存な項を除くと，結局，上で示した $E(\\bm w)$ を最小化することに相当します．\n 最尤推定以外の方法\n補足となりますが，最尤推定の他にも $\\bm w$ の推定方法が存在します． ベイズ推論では，式(4)に示す事後確率 $p(\\bm w|\\mathcal D)$ を推論します． また，式(4)に示す事後確率を最大化するような $\\bm w$ を推定値とする手法をMAP推定（事後確率最大化推定）と呼びます． 詳細についてはベイズ統計の理論と方法（1.3 さまざまな推測方法） が参考になります．\n  ベイズ線形回帰 本題のベイズ線形回帰の説明をします． 先程と同じように，ノイズは平均 $0$，分散 $\\sigma_{\\epsilon}^2$ のガウス分布に従うと仮定すると，尤度関数は式(5)のようになります． ベイズ線形回帰を行うために，回帰係数の事前分布 $p(\\bm w)$ を仮定する必要があります． 事前分布を，平均 $\\bm 0$，分散・共分散行列が $\\sigma_w^2I$のガウス分布\n$$ p(\\bm w) = \\mathcal N(\\bm w|\\bm 0,\\sigma_w^2I) \\tag{7} $$\nであるとします．\n詳細は省きますが，$\\bm w$ の事後分布 $p(\\bm w|\\mathcal D)$ は以下のガウス分布となることがわかります．\n$$ p(\\bm w|\\mathcal D) = \\mathcal N(\\bm w|\\hat{\\bm\\mu}, \\hat{\\Sigma}) \\tag{8} $$ ここで，\n$$ \\hat{\\Sigma}^{-1} = \\sigma_{\\epsilon}^{-2}\\sum_{n=1}^N\\bm\\phi(\\bm x_n)\\bm\\phi(\\bm x_n)^T + \\sigma_w^{-2}I \\tag{9} $$\n$$ \\hat{\\bm\\mu} = \\sigma_{\\epsilon}^{-2}\\hat{\\Sigma}\\sum_{n=1}^Ny_n\\bm\\phi(\\bm x_n) \\tag{10} $$\nです． このように，ベイズ線形回帰では，回帰係数 $\\bm w$ の事後確率 $p(\\bm w|\\mathcal D)$ が得られます． 点推定である，最尤推定やMAP推定とは異なり，確率分布が得られています． これにより，パラメータ $\\bm w$ の不確実性の評価が可能になります．\n 可解モデルについて\n今回は，ノイズと$\\bm w$ の事前分布 $p(\\bm w)$ をガウス分布と仮定することで，幸いにも事後分布が解析的に解ける「可解モデル」となりました． 「なぜガウス分布か？」という疑問を持った方は，本記事の内容を超えてしまいますが，「指数型分布とその共役分布」について調べてみるとより理解が深まると思います． また，ベイズ統計の理論と方法（1.2.3 計算できる例） が参考になります．\nしかしながら，現実問題に対して統計モデルを構築しようとすると，可解モデルの範疇には収まらないことがほとんどです． 多くの場合，解析的には解かせてくれません． より一般的に，データ $\\mathcal D$ と，パラメータ $\\theta$ でパラメトライズされた確率モデル $p(\\theta, \\mathcal D)$ について考えます．\n例えばマルコフ連鎖モンテカルロ法（MCMC：Markov Chain Monte Carlo）を用いて，事後分布 $p(\\theta|\\mathcal D)$ からのパラメータ $\\theta$ のサンプリングを実施することで事後分布を再現することがあります． MCMCを使うと，パラメータの積分を含む統計量をサンプリングによる期待値で近似計算することができます．\nまた，事後分布を推定する代わりに，解析し易い分布 $q(\\theta)$ を考え，事後分布と $q(\\theta)$ の形状が近くなるように（KL擬距離が最小となるように） $\\theta$ を学習することもあります(変分ベイズ法)．\n  簡単な例 では，簡単な例でベイズ線形回帰を実施してみましょう． 今回は，\n$$ y = ax+b \\tag{11} $$\nという1次関数について考えます． 真のモデルを $a^* = 0.5,~b^* = -1$ とします． 人工データについて説明します． $n$番目のデータについて，入力変数 $x_n\\in\\mathbb R^1$ は，区間 $[-5, 5]$ 上の一様分布からランダムに生成します． また，出力変数 $y_n\\in\\mathbb R^1$ は $y_n=a^*x_n+b^*+\\epsilon_n$ として生成します． ここで，$\\epsilon_n$ はガウスノイズであり，今回は平均 $\\mu_{\\epsilon} = 0$，分散 $\\sigma_{\\epsilon}^2=(0.3)^2$ としました． また，$a,b$それぞれの事前分布のガウス分布の平均は $\\mu_a=\\mu_b=0$ とし，分散は $\\sigma_a^2=\\sigma_b^2 = 1$ としました．\nデータ数を $N=10, 25, 50, 75, 100$ とした場合についてベイズ線形回帰を行い，$a, b$ の事後分布がどのような形になるのか観察してみます． まずはそれぞれのデータ数における観測データを以下に示します．\n最後に，得られた事後分布を確認してみましょう． データ数 $N = 10, 25, 50, 75, 100$ での事後分布と，追加でデータ数を $N=1,000$ まで増やした場合の事後分布を描写しています． 図中の $\\bm\\times$ は真の値 $(a^,b^)$ を示します． データ数が少ないときには，事後分布が大きな分散を持つガウス分布であることが確認できます． このケースでは，データ数の増加に伴い，事後分布の分散が小さくなり，真の切片，傾き付近にピークを持つような鋭いガウス分布となっていることがわかります．\nこの実験は以下のPythonスクリプトによって再現できます．\n コードはこちら # ライブラリのインポート（適宜インストールしてください） import numpy as np import matplotlib from matplotlib import pyplot as plt import matplotlib.mlab as mlab import japanize_matplotlib N = 10 # データ数 a = 0.5 # 真の傾き b = -1.0 # 真の切片 s_e = 1.0 # ノイズの従うガウス分布の分散 s_w = 1.0 # 事前分布の分散 np.random.seed(2022) # シードの固定 x = np.random.uniform(-5, 5, N) # N個の入力変数を生成 y = a * x + b + np.random.normal(0, s_e, N) # 出力変数を計算しノイズを加重 # 実験に使用する人工データの描写 fig = plt.figure(figsize = (8, 4), dpi = 200) ax = fig.add_subplot(111) ax.grid() ax.set_xlabel(\"入力変数 $x$\", fontsize = 18) ax.set_ylabel(\"出力変数 $y$\", fontsize = 18) # この辺はなんか綺麗になるように手動で設定 ax.set_xlim(-5 - 0.5, 5 + 0.5) ax.set_ylim(-5 * a + b - 0.5, 5 * a + b + 0.5) # 散布図の描写 ax.scatter(x, y, label = \"観測データ $\\mathcal{D}=(x_n,y_n)_{n=1}^N$\", color = \"gray\") # 真の直線の描写 ax.plot([-5.5, 5.5], [-5.5 * a + b, 5.5 * a + b], color = \"black\", ls = \"--\", zorder = -10, label = \"真の関数 $y=0.5x-1$\") ax.legend(fontsize = 16) ax.set_title(\"データ数 $N = {\"+str(N)+\"}$\", fontsize = 18) # 図の保存 # fig.savefig(\"fig/data_\"+str(N)+\".jpg\", bbox_inches = \"tight\", transparent=True) # 特徴ベクトルの作成 phi = np.concatenate([np.ones(shape = (N, 1)), x.reshape(-1, 1)], axis = 1) # 事後分布の共分散行列の計算 phiphiT = phi.T.dot(phi) Sigma_inv = phiphiT / (s_e**2) + np.eye(2) / (s_w**2) Sigma = np.linalg.inv(Sigma_inv) # 事後分布の平均の計算 mu = (Sigma / (s_e**2)).dot(phi.T.dot(y.reshape(-1, 1))) # 2次元ガウス分布の確率密度関数を計算する関数 def f(x, y, mu, S): x_norm = (np.array([x, y]) - mu[:, None, None]).transpose(1, 2, 0) return np.exp(- x_norm[:, :, None, :] @ np.linalg.inv(S)[None, None, :, :] @ x_norm[:, :, :, None] / 2.0) / (2*np.pi*np.sqrt(np.linalg.det(S))) # グリッドの定義 x = np.linspace(-1.5, -0.5, 200) y = np.linspace(0.25, 0.75, 200) X, Y = np.meshgrid(x, y) # 確率密度関数の計算 Z = f(X, Y, mu = mu.squeeze(), S = Sigma).squeeze() # 確率密度関数の等高線の描写 fig = plt.figure(figsize = (6, 6), dpi = 200) ax = fig.add_subplot(111) #1. 0.75 0.5 - 0.375, 0.5 + 0.375 # 等高線の描写 ax.contour(X, Y, Z, cmap = plt.cm.get_cmap(\"Greys\")) # 真の傾きと切片を描写 ax.scatter(b, a, label = \"$(a^*,b^*) = (0.5, -1)$\", marker = \"x\", s = 75, zorder = 10, lw = 2.5, color = \"black\") ax.set_xlabel(\"切片 $b$\", fontsize = 18) ax.set_ylabel(\"傾き $a$\", fontsize = 18) ax.legend(fontsize = 16) ax.grid({'grid_alpha' : 0.25}) ax.set_title(\"データ数 $N={\" +str(N)+ \"}$\", fontsize = 18) # 図の保存 # fig.savefig(\"fig/post_\" + str(N) + \".jpg\", bbox_inches = \"tight\", transparent=True)  まとめ 本記事では，ベイズ線形回帰をおさらいしました． 具体的には，線形回帰モデルにおいて，ノイズにガウス分布，パラメータの事前分布にガウス分布を仮定した場合の事後分布を示すとともに，１次関数による簡単な実験を行いました．\nというか，修士論文の審査前日に何をやっているのでしょうか． 僕は果たして大丈夫なのでしょうか． というわけで，訪問ありがとうございました．\n 参考文献 [1] ベイズ統計の理論と方法（渡辺澄夫 著, コロナ社）\n","wordCount":"811","inLanguage":"en","datePublished":"2022-01-30T00:00:00Z","dateModified":"2022-01-30T00:00:00Z","author":[{"@type":"Person","name":"TH"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://t0m0ya1997.github.io/thblog/posts/bayes_lr/"},"publisher":{"@type":"Organization","name":"TH's Blog","logo":{"@type":"ImageObject","url":"https://t0m0ya1997.github.io/thblog/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body class=dark id=top>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://t0m0ya1997.github.io/thblog/ accesskey=h title="Home (Alt + H)">
<img src=https://t0m0ya1997.github.io/thblog/images/home.svg alt=logo aria-label=logo height=40>Home</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu>
<li>
<a href=https://t0m0ya1997.github.io/thblog/archives/ title=記事一覧>
<span>記事一覧</span>
</a>
</li>
<li>
<a href=https://t0m0ya1997.github.io/thblog/categories/ title=カテゴリー>
<span>カテゴリー</span>
</a>
</li>
<li>
<a href=https://t0m0ya1997.github.io/thblog/tags/ title=タグ>
<span>タグ</span>
</a>
</li>
</ul>
</nav>
</header>
<div id=vanta-bg></div>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://t0m0ya1997.github.io/thblog/>Home</a>&nbsp;»&nbsp;<a href=https://t0m0ya1997.github.io/thblog/posts/>Posts</a></div>
<h1 class=post-title>
ベイズ線形回帰
</h1>
<div class=post-description>
この記事では，ベイズ線形回帰を紹介します．
</div>
<div class=post-meta><span title="2022-01-30 00:00:00 +0000 UTC">January 30, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;TH&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/bayes_lr/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<div class=post-content><p>修士論文審査の前日にして，特に発表には関係のない記事を書きました．
僕には，試験前，締め切り前，など切羽詰まった課題の前に立たされると，その課題以外の課題に猛烈に興味を持ったり，普段は開かない，随分前に読んだ本を手に取り，ついつい読んでしまうことが多々あります．
今日もそんな感じです．</p>
<p>この記事では，ベイズ線形回帰を紹介します．
線形回帰モデルとは，$d\in\mathbb N$ 次元入力 $\bm x\in\mathbb R^d$ と回帰係数 $\bm w^d$ の線形結合で実数出力 $y\in\mathbb R$ を予測するモデルで，ノイズを $\epsilon\in\mathbb R$ として，以下の関係を仮定します．</p>
<p>$$
y = \bm w^T\bm x + \epsilon \tag{1}
$$</p>
<p>これは，$d=1$ のとき「<strong>単回帰</strong>」，$d=2$以上のとき「<strong>重回帰</strong>」と呼ばれるモデルです．
また，入力変数 $\bm x$ の $m\in\mathbb N$ 個の関数値による特徴ベクトル $\bm \phi(\bm x) = (\phi_1(\bm x),\phi_2(\bm x),\cdots,\phi_m(\bm x))^T$ に対して回帰係数 $\bm w\in\mathbb R^m$ を仮定し，</p>
<p>$$
y = \bm w^T\bm \phi(\bm x) + \epsilon \tag{2}
$$</p>
<p>として $\bm w$ と $\bm\phi(\bm x)$ の線型結合で出力変数を予測することもあります．
これもまた，線形回帰と呼ばれます．
上述した<strong>単回帰</strong>および<strong>重回帰</strong>は，$\bm\phi(\bm x) = \bm x$ とした場合であることがわかります．</p>
<p>前提として，線形回帰モデルでは，「<mark>入力と出力の関係を表す回帰係数 $\bm w$ を良い感じに定める</mark>」ことが目標となります．</p>
<hr>
<h2 id=線形回帰モデルの学習>線形回帰モデルの学習<a hidden class=anchor aria-hidden=true href=#線形回帰モデルの学習>#</a></h2>
<p>$N\in\mathbb R$ 個の入力・出力の組 $\mathcal D = (\bm x_n,y_n)_{n=1}^N$ が与えられたとします．
このとき，以下の二乗誤差 $E(\bm w)$ を最小化するように $\bm w$ を決定することがあります．</p>
<p>$$
E(\bm w) = \frac{1}{2}\sum_{n=1}^N\left\{y_n - \bm w^T\bm \phi(\bm x_n)\right\}^2 \tag{3}
$$</p>
<p>これは，<strong>最小二乗法</strong>と呼ばれ，一度は耳にしたことがあるかもしれません．
この最小二乗法で得られる $\bm w$ は，<mark>ノイズがガウス分布に従うと仮定したもとでの最尤推定量 $\bm w_{ML}$ と一致すること</mark>が次から分ります．</p>
<p>まず，便宜的に 入力・出力のデータ $\mathcal D$ が与えられたもとでの，$\bm w$ の確率分布 $p(\bm w|\mathcal D)$ を仮定します．
これを，$\mathcal D$ を観測したもとでの $\bm w$ の<strong>事後確率/事後分布</strong>と呼びます．
ベイズの定理から</p>
<p>$$
p(\bm w|\mathcal D) = \frac{p(\mathcal D|\bm w)p(\bm w)}{p(\mathcal D)} \propto p(\mathcal D|\bm w)p(\bm w) \tag{4}
$$</p>
<p>が得られます．
ここで，$p(\mathcal D|\bm w)$ は<strong>尤度関数</strong>，$p(\bm w)$ は<strong>事前確率/事前分布</strong>と呼ばれます．
最尤推定とは，尤度関数を最大化させる $\bm w$ を求めることを指します．
では，ノイズが平均 $0$, 分散 $\sigma_{\epsilon}^2$ のガウス分布に独立に従うと仮定すると，尤度関数は</p>
<p>$$
\begin{aligned}
p(\mathcal D|\bm w)
& = \prod_{n=1}^N\mathcal N(\epsilon_n|0,\sigma_{\epsilon}^2)
= \prod_{n=1}^N\mathcal N(y_n-\bm w^T\bm\phi(\bm x_n)|0,\sigma_{\epsilon}^2) \cr
& = (2\pi\sigma_{\epsilon}^2)^{-N/2}\prod_{n=1}^N\exp\left(\frac{(y_n-\bm w^T\bm\phi(\bm x_n))^2}{2\sigma_{\epsilon}^2}\right)
\end{aligned}
\tag{5}
$$</p>
<p>となります．
この尤度関数を最大化することは，対数をとり符号反転をした $-\log p(\mathcal D|\bm w)$ を $\bm w$ について最小化することと等価です．
つまり，</p>
<p>$$
-\log p(\mathcal D|\bm w) = \frac{1}{2\sigma^2}\sum_{n=1}^N\left\{y_n-\bm w^T\bm\phi(\bm x_n)\right\}^2 + \frac{N}{2}\log(2\pi\sigma^2) \tag{6}
$$</p>
<p>を $\bm w$ について最小化することと等価です．
この式の $\bm w$ に非依存な項を除くと，結局，上で示した $E(\bm w)$ を最小化することに相当します．</p>
<blockquote>
<p>最尤推定以外の方法</p>
<p>補足となりますが，最尤推定の他にも $\bm w$ の推定方法が存在します．
<strong>ベイズ推論</strong>では，式(4)に示す事後確率 $p(\bm w|\mathcal D)$ を推論します．
また，式(4)に示す事後確率を最大化するような $\bm w$ を推定値とする手法を<strong>MAP推定</strong>（事後確率最大化推定）と呼びます．
詳細については<a href=(https://www.coronasha.co.jp/np/isbn/9784339024623/)>ベイズ統計の理論と方法</a>（1.3 さまざまな推測方法） が参考になります．</p>
</blockquote>
<hr>
<h2 id=ベイズ線形回帰>ベイズ線形回帰<a hidden class=anchor aria-hidden=true href=#ベイズ線形回帰>#</a></h2>
<p>本題のベイズ線形回帰の説明をします．
先程と同じように，ノイズは平均 $0$，分散 $\sigma_{\epsilon}^2$ のガウス分布に従うと仮定すると，尤度関数は式(5)のようになります．
ベイズ線形回帰を行うために，回帰係数の事前分布 $p(\bm w)$ を仮定する必要があります．
事前分布を，平均 $\bm 0$，分散・共分散行列が $\sigma_w^2I$のガウス分布</p>
<p>$$
p(\bm w) = \mathcal N(\bm w|\bm 0,\sigma_w^2I)
\tag{7}
$$</p>
<p>であるとします．</p>
<p>詳細は省きますが，$\bm w$ の事後分布 $p(\bm w|\mathcal D)$ は以下のガウス分布となることがわかります．</p>
<p>$$
p(\bm w|\mathcal D) = \mathcal N(\bm w|\hat{\bm\mu}, \hat{\Sigma})
\tag{8}
$$
ここで，</p>
<p>$$
\hat{\Sigma}^{-1} = \sigma_{\epsilon}^{-2}\sum_{n=1}^N\bm\phi(\bm x_n)\bm\phi(\bm x_n)^T + \sigma_w^{-2}I
\tag{9}
$$</p>
<p>$$
\hat{\bm\mu} = \sigma_{\epsilon}^{-2}\hat{\Sigma}\sum_{n=1}^Ny_n\bm\phi(\bm x_n)
\tag{10}
$$</p>
<p>です．
このように，ベイズ線形回帰では，回帰係数 $\bm w$ の事後確率 $p(\bm w|\mathcal D)$ が得られます．
点推定である，<strong>最尤推定</strong>や<strong>MAP推定</strong>とは異なり，確率分布が得られています．
これにより，パラメータ $\bm w$ の不確実性の評価が可能になります．</p>
<blockquote>
<p>可解モデルについて</p>
<p>今回は，ノイズと$\bm w$ の事前分布 $p(\bm w)$ をガウス分布と仮定することで，幸いにも事後分布が解析的に解ける「<strong>可解モデル</strong>」となりました．
「なぜガウス分布か？」という疑問を持った方は，本記事の内容を超えてしまいますが，「指数型分布とその共役分布」について調べてみるとより理解が深まると思います．
また，<a href=(https://www.coronasha.co.jp/np/isbn/9784339024623/)>ベイズ統計の理論と方法</a>（1.2.3 計算できる例） が参考になります．</p>
<p>しかしながら，現実問題に対して統計モデルを構築しようとすると，可解モデルの範疇には収まらないことがほとんどです．
多くの場合，解析的には解かせてくれません．
より一般的に，データ $\mathcal D$ と，パラメータ $\theta$ でパラメトライズされた確率モデル $p(\theta, \mathcal D)$ について考えます．</p>
<p>例えばマルコフ連鎖モンテカルロ法（<strong>MCMC</strong>：Markov Chain Monte Carlo）を用いて，事後分布 $p(\theta|\mathcal D)$ からのパラメータ $\theta$ のサンプリングを実施することで事後分布を再現することがあります．
MCMCを使うと，パラメータの積分を含む統計量をサンプリングによる期待値で近似計算することができます．</p>
<p>また，事後分布を推定する代わりに，解析し易い分布 $q(\theta)$ を考え，事後分布と $q(\theta)$ の形状が近くなるように（KL擬距離が最小となるように） $\theta$ を学習することもあります(<strong>変分ベイズ法</strong>)．</p>
</blockquote>
<hr>
<h2 id=簡単な例>簡単な例<a hidden class=anchor aria-hidden=true href=#簡単な例>#</a></h2>
<p>では，簡単な例でベイズ線形回帰を実施してみましょう．
今回は，</p>
<p>$$
y = ax+b
\tag{11}
$$</p>
<p>という1次関数について考えます．
真のモデルを $a^* = 0.5,~b^* = -1$ とします．
人工データについて説明します．
$n$番目のデータについて，入力変数 $x_n\in\mathbb R^1$ は，区間 $[-5, 5]$ 上の一様分布からランダムに生成します．
また，出力変数 $y_n\in\mathbb R^1$ は $y_n=a^*x_n+b^*+\epsilon_n$ として生成します．
ここで，$\epsilon_n$ はガウスノイズであり，今回は平均 $\mu_{\epsilon} = 0$，分散 $\sigma_{\epsilon}^2=(0.3)^2$ としました．
また，$a,b$それぞれの事前分布のガウス分布の平均は $\mu_a=\mu_b=0$ とし，分散は $\sigma_a^2=\sigma_b^2 = 1$ としました．</p>
<p>データ数を $N=10, 25, 50, 75, 100$ とした場合についてベイズ線形回帰を行い，$a, b$ の事後分布がどのような形になるのか観察してみます．
まずはそれぞれのデータ数における観測データを以下に示します．</p>
<p><img loading=lazy src=data.jpg alt>
</p>
<p>最後に，得られた事後分布を確認してみましょう．
データ数 $N = 10, 25, 50, 75, 100$ での事後分布と，追加でデータ数を $N=1,000$ まで増やした場合の事後分布を描写しています．
図中の $\bm\times$ は真の値 $(a^<em>,b^</em>)$ を示します．
データ数が少ないときには，事後分布が大きな分散を持つガウス分布であることが確認できます．
このケースでは，データ数の増加に伴い，事後分布の分散が小さくなり，真の切片，傾き付近にピークを持つような鋭いガウス分布となっていることがわかります．</p>
<p><img loading=lazy src=post.jpg alt>
</p>
<p>この実験は以下の<code>Python</code>スクリプトによって再現できます．</p>
<details>
<summary>コードはこちら</summary>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
    <span style=color:#75715e># ライブラリのインポート（適宜インストールしてください）</span>
    <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
    <span style=color:#f92672>import</span> matplotlib
    <span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
    <span style=color:#f92672>import</span> matplotlib.mlab <span style=color:#66d9ef>as</span> mlab
    <span style=color:#f92672>import</span> japanize_matplotlib

    N <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span> <span style=color:#75715e># データ数</span>

    a <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span> <span style=color:#75715e># 真の傾き</span>
    b <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span> <span style=color:#75715e># 真の切片</span>

    s_e <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#75715e># ノイズの従うガウス分布の分散</span>
    s_w <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#75715e># 事前分布の分散</span>

    np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>2022</span>) <span style=color:#75715e># シードの固定</span>
    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>, N) <span style=color:#75715e># N個の入力変数を生成</span>
    y <span style=color:#f92672>=</span> a <span style=color:#f92672>*</span> x <span style=color:#f92672>+</span> b <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, s_e, N) <span style=color:#75715e># 出力変数を計算しノイズを加重</span>

    <span style=color:#75715e># 実験に使用する人工データの描写</span>
    fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(figsize <span style=color:#f92672>=</span> (<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>4</span>), dpi <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>)
    ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>)
    ax<span style=color:#f92672>.</span>grid()
    ax<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#34;入力変数 $x$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)
    ax<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#34;出力変数 $y$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)

    <span style=color:#75715e># この辺はなんか綺麗になるように手動で設定</span>
    ax<span style=color:#f92672>.</span>set_xlim(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>5</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span>)
    ax<span style=color:#f92672>.</span>set_ylim(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> a <span style=color:#f92672>+</span> b <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> a <span style=color:#f92672>+</span> b <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span>)

    <span style=color:#75715e># 散布図の描写</span>
    ax<span style=color:#f92672>.</span>scatter(x, y, label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;観測データ $\mathcal</span><span style=color:#e6db74>{D}</span><span style=color:#e6db74>=(x_n,y_n)_{n=1}^N$&#34;</span>, color <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;gray&#34;</span>)
    <span style=color:#75715e># 真の直線の描写</span>
    ax<span style=color:#f92672>.</span>plot([<span style=color:#f92672>-</span><span style=color:#ae81ff>5.5</span>, <span style=color:#ae81ff>5.5</span>], [<span style=color:#f92672>-</span><span style=color:#ae81ff>5.5</span> <span style=color:#f92672>*</span> a <span style=color:#f92672>+</span> b, <span style=color:#ae81ff>5.5</span> <span style=color:#f92672>*</span> a <span style=color:#f92672>+</span> b], color <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;black&#34;</span>, ls <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;--&#34;</span>, zorder <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>, label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;真の関数 $y=0.5x-1$&#34;</span>)

    ax<span style=color:#f92672>.</span>legend(fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>)
    ax<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#34;データ数 $N = {&#34;</span><span style=color:#f92672>+</span>str(N)<span style=color:#f92672>+</span><span style=color:#e6db74>&#34;}$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)

    <span style=color:#75715e># 図の保存</span>
    <span style=color:#75715e># fig.savefig(&#34;fig/data_&#34;+str(N)+&#34;.jpg&#34;, bbox_inches = &#34;tight&#34;, transparent=True)</span>

    <span style=color:#75715e># 特徴ベクトルの作成</span>
    phi <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([np<span style=color:#f92672>.</span>ones(shape <span style=color:#f92672>=</span> (N, <span style=color:#ae81ff>1</span>)), x<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)], axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)

    <span style=color:#75715e># 事後分布の共分散行列の計算</span>
    phiphiT <span style=color:#f92672>=</span> phi<span style=color:#f92672>.</span>T<span style=color:#f92672>.</span>dot(phi)
    Sigma_inv <span style=color:#f92672>=</span> phiphiT <span style=color:#f92672>/</span> (s_e<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>/</span> (s_w<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)
    Sigma <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(Sigma_inv)

    <span style=color:#75715e># 事後分布の平均の計算</span>
    mu <span style=color:#f92672>=</span> (Sigma <span style=color:#f92672>/</span> (s_e<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>))<span style=color:#f92672>.</span>dot(phi<span style=color:#f92672>.</span>T<span style=color:#f92672>.</span>dot(y<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)))

    <span style=color:#75715e># 2次元ガウス分布の確率密度関数を計算する関数</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x, y, mu, S):
    x_norm <span style=color:#f92672>=</span> (np<span style=color:#f92672>.</span>array([x, y]) <span style=color:#f92672>-</span> mu[:, <span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>None</span>])<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span> x_norm[:, :, <span style=color:#66d9ef>None</span>, :] <span style=color:#f92672>@</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(S)[<span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>None</span>, :, :] <span style=color:#f92672>@</span> x_norm[:, :, :, <span style=color:#66d9ef>None</span>] <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span>) <span style=color:#f92672>/</span> (<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>pi<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>det(S)))

    <span style=color:#75715e># グリッドの定義</span>
    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>1.5</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>200</span>)
    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.75</span>, <span style=color:#ae81ff>200</span>)
    X, Y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(x, y)

    <span style=color:#75715e># 確率密度関数の計算</span>
    Z <span style=color:#f92672>=</span> f(X, Y, mu <span style=color:#f92672>=</span> mu<span style=color:#f92672>.</span>squeeze(), S <span style=color:#f92672>=</span> Sigma)<span style=color:#f92672>.</span>squeeze()

    <span style=color:#75715e># 確率密度関数の等高線の描写</span>
    fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(figsize <span style=color:#f92672>=</span> (<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>6</span>), dpi <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span>)
    ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>) <span style=color:#75715e>#1. 0.75 0.5 - 0.375, 0.5 + 0.375</span>

    <span style=color:#75715e># 等高線の描写</span>
    ax<span style=color:#f92672>.</span>contour(X, Y, Z, cmap <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>cm<span style=color:#f92672>.</span>get_cmap(<span style=color:#e6db74>&#34;Greys&#34;</span>))
    <span style=color:#75715e># 真の傾きと切片を描写</span>
    ax<span style=color:#f92672>.</span>scatter(b, a, label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;$(a^*,b^*) = (0.5, -1)$&#34;</span>, marker <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;x&#34;</span>, s <span style=color:#f92672>=</span> <span style=color:#ae81ff>75</span>, zorder <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>, lw <span style=color:#f92672>=</span> <span style=color:#ae81ff>2.5</span>, color <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;black&#34;</span>)

    ax<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#34;切片 $b$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)
    ax<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#34;傾き $a$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)
    ax<span style=color:#f92672>.</span>legend(fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>)
    ax<span style=color:#f92672>.</span>grid({<span style=color:#e6db74>&#39;grid_alpha&#39;</span> : <span style=color:#ae81ff>0.25</span>})
    ax<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#34;データ数 $N={&#34;</span> <span style=color:#f92672>+</span>str(N)<span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;}$&#34;</span>, fontsize <span style=color:#f92672>=</span> <span style=color:#ae81ff>18</span>)

    <span style=color:#75715e># 図の保存</span>
    <span style=color:#75715e># fig.savefig(&#34;fig/post_&#34; + str(N) + &#34;.jpg&#34;, bbox_inches = &#34;tight&#34;, transparent=True)</span>

</code></pre></div></details>
<h2 id=まとめ>まとめ<a hidden class=anchor aria-hidden=true href=#まとめ>#</a></h2>
<p>本記事では，ベイズ線形回帰をおさらいしました．
具体的には，線形回帰モデルにおいて，ノイズにガウス分布，パラメータの事前分布にガウス分布を仮定した場合の事後分布を示すとともに，１次関数による簡単な実験を行いました．</p>
<p>というか，修士論文の審査前日に何をやっているのでしょうか．
僕は果たして大丈夫なのでしょうか．
というわけで，訪問ありがとうございました．</p>
<hr>
<h4 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h4>
<p>[1] <a href=https://www.coronasha.co.jp/np/isbn/9784339024623/>ベイズ統計の理論と方法</a>（渡辺澄夫 著, コロナ社）</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://t0m0ya1997.github.io/thblog/tags/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96/>ベイズ推論</a></li>
<li><a href=https://t0m0ya1997.github.io/thblog/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/>機械学習</a></li>
<li><a href=https://t0m0ya1997.github.io/thblog/tags/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0/>線形回帰</a></li>
<li><a href=https://t0m0ya1997.github.io/thblog/tags/c++/>C++</a></li>
<li><a href=https://t0m0ya1997.github.io/thblog/tags/python/>Python</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://t0m0ya1997.github.io/thblog/posts/keiba/1/>
<span class=title>« Prev Page</span>
<br>
<span>100日後に競馬AIを構築する話(0)</span>
</a>
<a class=next href=https://t0m0ya1997.github.io/thblog/posts/master_thesis/>
<span class=title>Next Page »</span>
<br>
<span>修士論文を提出しました。もう一度洗濯して、干します。</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0&url=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f&hashtags=%e3%83%99%e3%82%a4%e3%82%ba%e6%8e%a8%e8%ab%96%2c%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92%2c%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0%2cC%2b%2b%2cPython"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f&title=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0&summary=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0&source=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f&title=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0%20-%20https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ベイズ線形回帰 on telegram" href="https://telegram.me/share/url?text=%e3%83%99%e3%82%a4%e3%82%ba%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0&url=https%3a%2f%2ft0m0ya1997.github.io%2fthblog%2fposts%2fbayes_lr%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://t0m0ya1997.github.io/thblog/>TH's Blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>,
<a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>, and
<a href=https://www.vantajs.com rel=noopener target=_blank>Vanta</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script src=https://cdnjs.cloudflare.com/ajax/libs/three.js/r121/three.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/vanta@0.5.21/dist/vanta.cells.min.js></script>
<script>const effect=VANTA.CELLS({el:"#vanta-bg",mouseControls:!0,touchControls:!0,gyroControls:!1,minHeight:200,minWidth:200,scale:8,color1:0,color2:1916754,size:1,speed:1})</script>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body>
</html>